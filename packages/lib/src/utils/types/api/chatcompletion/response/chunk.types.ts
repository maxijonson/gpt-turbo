import { ChatCompletionResponseMessageLogprob } from "./logprob.types.js";
import { ChatCompletionResponseMessage } from "./message.types.js";

/**
 * The response body for a single chunk generated by the {@link ChatCompletionRequest Create Chat Completion request} request with `stream` set to `true`.
 */
export interface ChatCompletionResponseCunk {
    /**
     * A unique identifier for the chat completion.
     */
    id: string;

    /**
     * The object type, which is always `chat_completion`.
     */
    object: string;

    /**
     * The time the chat completion was created, in Unix time.
     */
    created: number;

    /**
     * The model used to generate the chat completion.
     */
    model: string;

    /**
     * This fingerprint represents the backend configuration that the model runs with.
     * Can be used in conjunction with the seed request parameter to understand when backend changes have been made that might impact determinism.
     */
    system_fingerprint: string;

    /**
     * The chat completion choices generated by the model. Always contains exactly one choice.
     */
    choices: [
        {
            /**
             * A token representing part of the assistant's response to the prompt.
             */
            delta: Omit<ChatCompletionResponseMessage, "logprobs">;

            /**
             * Log probability information for the choice.
             */
            logprobs: ChatCompletionResponseMessageLogprob | null;

            /**
             * The reason the chat completion ended. `null` when streaming, otherwise "stop".
             */
            finish_reason: string | null;

            /**
             * The index of the choice in the request. Always 0.
             */
            index: number;
        },
    ];
}
