import { ChatCompletionResponseMessage } from "./message.types.js";

/**
 * The response body for a {@link CreateChatCompletionRequest Create Chat Completion request}.
 *
 * @see {@link https://platform.openai.com/docs/guides/chat/response-format Chat Completion Response Body}
 */
export interface ChatCompletionResponse {
    /**
     * A unique identifier for the chat completion.
     */
    id: string;

    /**
     * The chat completion choices generated by the model. Always contains exactly one choice.
     */
    choices: [
        {
            /**
             * A chat completion message generated by the model.
             */
            message: ChatCompletionResponseMessage;

            /**
             * The reason the chat completion ended. Always "stop" for non-streamed completions.
             */
            finish_reason: string;

            /**
             * The index of the choice in the request. Always 0.
             */
            index: number;
        },
    ];

    /**
     * The object type, which is always `chat_completion`.
     */
    object: string;

    /**
     * The time the chat completion was created, in Unix time.
     */
    created: number;

    /**
     * The model used to generate the chat completion.
     */
    model: string;

    /**
     * This fingerprint represents the backend configuration that the model runs with.
     *
     * Can be used in conjunction with the `seed` request parameter to understand when backend changes have been made that might impact determinism.
     */
    system_fingerprint: string;

    /**
     * The usage incurred by the chat completion.
     */
    usage: {
        /**
         * The number of tokens used for the prompt.
         */
        prompt_tokens: number;

        /**
         * The number of tokens used for the completion.
         */
        completion_tokens: number;

        /**
         * The sum of the prompt and completion tokens.
         */
        total_tokens: number;
    };
}
